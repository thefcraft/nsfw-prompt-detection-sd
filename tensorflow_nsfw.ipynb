{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Load the data\n",
    "with open('./preproced_new_data.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the prompts and negative prompts\n",
    "prompts = [d['prompt'] for d in data['items']]\n",
    "neg_prompts = [d['negativePrompt'] for d in data['items']]\n",
    "labels = [d['nsfw'] for d in data['items']]\n",
    "\n",
    "# Define the vocabulary size and embedding dimensions\n",
    "vocab_size = 10000\n",
    "embedding_dim = 64\n",
    "\n",
    "# Tokenize the prompts and negative prompts\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(prompts + neg_prompts)\n",
    "prompt_sequences = tokenizer.texts_to_sequences(prompts)\n",
    "neg_prompt_sequences = tokenizer.texts_to_sequences(neg_prompts)\n",
    "\n",
    "# Pad the prompt and negative prompt sequences\n",
    "max_sequence_length = 50\n",
    "prompt_padded = tf.keras.preprocessing.sequence.pad_sequences(prompt_sequences, maxlen=max_sequence_length, truncating='post', padding='post')\n",
    "neg_prompt_padded = tf.keras.preprocessing.sequence.pad_sequences(neg_prompt_sequences, maxlen=max_sequence_length, truncating='post', padding='post')\n",
    "\n",
    "# Convert the labels to numpy arrays\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Shuffle the data\n",
    "indices = np.arange(len(prompts))\n",
    "np.random.shuffle(indices)\n",
    "prompt_padded = prompt_padded[indices]\n",
    "neg_prompt_padded = neg_prompt_padded[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "split = 0.8\n",
    "split_index = int(len(prompts) * split)\n",
    "x_train_prompt = prompt_padded[:split_index]\n",
    "x_train_neg_prompt = neg_prompt_padded[:split_index]\n",
    "y_train = labels[:split_index]\n",
    "x_val_prompt = prompt_padded[split_index:]\n",
    "x_val_neg_prompt = neg_prompt_padded[split_index:]\n",
    "y_val = labels[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input shapes\n",
    "prompt_input_shape = (max_sequence_length,)\n",
    "neg_prompt_input_shape = (max_sequence_length,)\n",
    "\n",
    "# Define the input layers\n",
    "prompt_input_layer = tf.keras.layers.Input(shape=prompt_input_shape, name='prompt_input')\n",
    "neg_prompt_input_layer = tf.keras.layers.Input(shape=neg_prompt_input_shape, name='neg_prompt_input')\n",
    "\n",
    "# Define the embedding layers\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True, name='embedding_layer')\n",
    "\n",
    "# Define the LSTM layer\n",
    "lstm_units = 64\n",
    "lstm_layer = tf.keras.layers.LSTM(units=lstm_units, name='lstm_layer')\n",
    "\n",
    "# Define the output layer\n",
    "output_layer = tf.keras.layers.Dense(units=1, activation='sigmoid', name='output_layer')\n",
    "\n",
    "# Pass the prompt and neg_prompt inputs through the embedding layer and LSTM layer\n",
    "prompt_embedded = embedding_layer(prompt_input_layer)\n",
    "neg_prompt_embedded = embedding_layer(neg_prompt_input_layer)\n",
    "\n",
    "prompt_lstm_output = lstm_layer(prompt_embedded)\n",
    "neg_prompt_lstm_output = lstm_layer(neg_prompt_embedded)\n",
    "\n",
    "# Concatenate the LSTM outputs\n",
    "concatenated_output = tf.keras.layers.concatenate([prompt_lstm_output, neg_prompt_lstm_output], axis=-1)\n",
    "\n",
    "# Pass the concatenated output through the output layer\n",
    "model_output = output_layer(concatenated_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model inputs and outputs\n",
    "model_inputs = [prompt_input_layer, neg_prompt_input_layer]\n",
    "model_outputs = model_output\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.models.Model(inputs=model_inputs, outputs=model_outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5247/5247 [==============================] - 567s 105ms/step - loss: 0.4267 - accuracy: 0.8053 - val_loss: 0.3953 - val_accuracy: 0.8192\n",
      "Epoch 2/10\n",
      "5247/5247 [==============================] - 489s 93ms/step - loss: 0.3715 - accuracy: 0.8334 - val_loss: 0.3870 - val_accuracy: 0.8253\n",
      "Epoch 3/10\n",
      "5247/5247 [==============================] - 487s 93ms/step - loss: 0.3429 - accuracy: 0.8468 - val_loss: 0.3825 - val_accuracy: 0.8300\n",
      "Epoch 4/10\n",
      "5247/5247 [==============================] - 477s 91ms/step - loss: 0.3199 - accuracy: 0.8584 - val_loss: 0.3944 - val_accuracy: 0.8309\n",
      "Epoch 5/10\n",
      "5247/5247 [==============================] - 505s 96ms/step - loss: 0.3003 - accuracy: 0.8676 - val_loss: 0.3891 - val_accuracy: 0.8320\n",
      "Epoch 6/10\n",
      "5247/5247 [==============================] - 484s 92ms/step - loss: 0.2820 - accuracy: 0.8760 - val_loss: 0.4008 - val_accuracy: 0.8320\n",
      "Epoch 7/10\n",
      "5247/5247 [==============================] - 502s 96ms/step - loss: 0.2658 - accuracy: 0.8833 - val_loss: 0.4207 - val_accuracy: 0.8331\n",
      "Epoch 8/10\n",
      "5247/5247 [==============================] - 487s 93ms/step - loss: 0.2511 - accuracy: 0.8900 - val_loss: 0.4168 - val_accuracy: 0.8324\n",
      "Epoch 9/10\n",
      "5247/5247 [==============================] - 488s 93ms/step - loss: 0.2380 - accuracy: 0.8957 - val_loss: 0.4305 - val_accuracy: 0.8298\n",
      "Epoch 10/10\n",
      "5247/5247 [==============================] - 502s 96ms/step - loss: 0.2258 - accuracy: 0.9006 - val_loss: 0.4591 - val_accuracy: 0.8297\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit([x_train_prompt, x_train_neg_prompt], y_train, validation_split=0.2, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1640/1640 [==============================] - 28s 17ms/step - loss: 0.4548 - accuracy: 0.8309\n",
      "Test loss: 0.454784095287323\n",
      "Test accuracy: 0.8308905959129333\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on test set\n",
    "loss, accuracy = model.evaluate([x_val_prompt, x_val_neg_prompt], y_val, batch_size=32)\n",
    "print(\"Test loss:\", loss)\n",
    "print(\"Test accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save('nsfw_classifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('nsfw_classifier_tokenizer.pickle', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nsfw_classifier.pickle', 'wb') as f:\n",
    "    pickle.dump(model, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess(text, isfirst = True):\n",
    "    if isfirst:\n",
    "        if type(text) == str: pass\n",
    "        elif type(text) == list:\n",
    "            output = []\n",
    "            for i in text:\n",
    "                output.append(preprocess(i))\n",
    "            return(output)\n",
    "            \n",
    "\n",
    "    text = re.sub('<.*?>', '', text)\n",
    "    text = re.sub('\\(+', '(', text)\n",
    "    text = re.sub('\\)+', ')', text)\n",
    "    matchs = re.findall('\\(.*?\\)', text)\n",
    "    \n",
    "    for _ in matchs:\n",
    "        text = text.replace(_, preprocess(_[1:-1], isfirst=False) )\n",
    "\n",
    "    text = text.replace('\\n', ',').replace('|',',')\n",
    "\n",
    "    if isfirst: \n",
    "        output = text.split(',')\n",
    "        output = list(map(lambda x: x.strip(), output))\n",
    "        output = [x for x in output if x != '']\n",
    "        return ', '.join(output)\n",
    "        # return output\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(prompts, negative_prompts, outputs, print_percentage = True):\n",
    "    for idx, i in enumerate(prompts):\n",
    "        print('*****************************************************************')\n",
    "        if print_percentage:\n",
    "            print(f\"prompt: {i}\\nnegative_prompt: {negative_prompts[idx]}\\npredict: {outputs[idx][0]} --{outputs[idx][1]}%\")\n",
    "        else:\n",
    "            print(f\"prompt: {i}\\nnegative_prompt: {negative_prompts[idx]}\\npredict: {outputs[idx][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 70ms/step\n",
      "Prediction: [('SFW', 100.0), ('NSFW', 99.44)]\n",
      "*****************************************************************\n",
      "prompt: a landscape with trees and mountains in the background\n",
      "negative_prompt: nsfw\n",
      "predict: SFW --100.0%\n",
      "*****************************************************************\n",
      "prompt: nude, sexy, 1girl, nsfw\n",
      "negative_prompt: worst quality\n",
      "predict: NSFW --99.44%\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on new data\n",
    "prompt = [\"a landscape with trees and mountains in the background\", 'nude, sexy, 1girl, nsfw']\n",
    "negative_prompt = [\"nsfw\", 'worst quality']\n",
    "\n",
    "x_new = tokenizer.texts_to_sequences( preprocess(prompt) )\n",
    "z_new = tokenizer.texts_to_sequences( preprocess(negative_prompt) )\n",
    "x_new = tf.keras.preprocessing.sequence.pad_sequences(x_new, maxlen=max_sequence_length)\n",
    "z_new = tf.keras.preprocessing.sequence.pad_sequences(z_new, maxlen=max_sequence_length)\n",
    "y_new = model.predict([x_new, z_new])\n",
    "y_new = list(map(lambda x:(\"NSFW\", float(\"{:.2f}\".format(x[0]*100)) ) if x[0]>0.5 else (\"SFW\", float(\"{:.2f}\".format(100-x[0]*100))), y_new))\n",
    "\n",
    "\n",
    "print(\"Prediction:\", y_new)\n",
    "postprocess(prompt, negative_prompt, y_new, print_percentage=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
